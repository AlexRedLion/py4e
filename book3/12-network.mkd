Programas en red
================

Aunque muchos de los ejemplos en este libro se han enfocado en leer archivos
y buscar datos en ellos, hay muchas fuentes de información diferentes
si se tiene en cuenta el Internet.

En este capítulo fingiremos ser un navegador web y recuperaremos páginas
web utilizando el Protocolo de Transporte de Hipertexto (HyperText Transfer Protocol - HTTP). Luego revisaremos los datos de esas páginas web y
los analizaremos.

Protocolo de Transporte de Hipertexto - HTTP
--------------------------------------------

El protocolo de red que hace funcionar la web es en realidad bastante
simple, y existe un soporte integrado en Python llamado `sockets`, el
cual hace que resulte muy fácil realizar conexiones de red y recuperar
datos a través de esas conexiones desde un programa de Python.

Un socket es muy parecido a un archivo, a excepción de que un solo
proporciona una conexión de doble sentido entre dos programas. Es posible
tanto leer como escribir en el mismo socket. Si se escribe algo en un
socket, es enviado a la aplicación que está al otro lado del socket. Si
se lee desde el socket, se obtienen los datos que la otra aplicación
ha enviado.

Pero si intentas leer un socket cuando el programa que está del otro lado
del socket no ha enviado ningún dato, puedes sentarte y esperar. Si los programas
de ambos extremos del socket simplemente esperan por datos sin enviar nada,
van a esperar por mucho, mucho tiempo, así que una parte importante de la
comunicación de programas a través del internet consiste en tener algún tipo
de protocolo.

Un protocolo es un conjunto de reglas precisas que determinan quién va primero,
qué debe hacer, cuáles son las respuestas siguientes para ese mensaje, quién
envía a continuación y todo lo demás. En cierto sentido las aplicaciones
a ambos lados del socket están interpretando un baile y cada una de ellas
debe estar segura de no pisar los pies del otro.

Hay muchos documentos que describen esos protocolos de red. El
Protocolo de Transporte de Hipertext está descrito en el siguiente documento:

<https://www.w3.org/Protocols/rfc2616/rfc2616.txt>

Se trata de un documento largo y complejo de 176 páginas, con un montón de
detalles. Si lo encuentras interesante, siéntete libre de leerlo completo. Pero si
echas un vistazo alrededor de la página 36 del RFC2616, encontrarás la sintaxis para las
peticiones GET. Para pedir un documento a un servidor web, hacemos una conexión al
servidor `www.pr4e.org` en el puerto 80, y luego enviamos una línea como esta:

`GET http://data.pr4e.org/romeo.txt HTTP/1.0 `

en la cual el segundo parámetro es la página web que estamos solicitando, y a
continuación enviamos una línea en blanco. El servidor web responderá con una
cabecera que contiene información acerca del documento y una línea en blanco,
seguido por el contenido del documento.

El navegador web más sencillo del mundo
---------------------------------------

Quizá la manera más sencilla de demostrar cómo funciona el protocolo HTTP sea
escribir un programa en Python muy sencillo, que realiza una conexión a un
servidor web y sigue las reglas del protocolo HTTP para solicitar un documento
y mostrar lo que el servidor envía de regreso.

\VerbatimInput{../code3/socket1.py}

En primer lugar, el programa realiza una conexión al puerto 80 del servidor
[www.py4e.com](http://www.py4e.com). Puesto que nuestro programa está jugando
el rol de "navegador web", el protocolo HTTP dice que debemos enviar
el comando GET seguido de una línea en blanco. `\r\n` representa un
salto de línea (end of line, o EOL en inglés), así que `\r\n\r\n`
significa que no hay nada entre dos secuencias de salto de línea. Ese
es el equivalente de una línea en blanco.

![Conexión de un socket](height=2.0in@../images/socket)

Una vez que enviamos esa línea en blanco, escribimos un bucle que recibe
los datos desde el socket en bloques de 512 caracteres y los imprime en
pantalla hasta que no quedan más datos por leer (es decir, la llamada a
recv() devuelve una cadena vacía).

El programa produce la salida siguiente:

~~~~
HTTP/1.1 200 OK
Date: Wed, 11 Apr 2018 18:52:55 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Sat, 13 May 2017 11:22:22 GMT
ETag: "a7-54f6609245537"
Accept-Ranges: bytes
Content-Length: 167
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: text/plain

But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
~~~~

La salida comienza con la cabecera que el servidor envía para describir
el documento. Por ejemplo, la cabecera `Content-Type` indica que
el documento es un documento de texto sin formato (`text/plain`).

Después de que el servidor nos envía la cabecera, añade una línea en blanco
para indicar el final de la cabecera, y después envía los datos reales del
archivo *romeo.txt*.

Este ejemplo muestra cómo hacer una conexión de red de bajo nivel con
sockets. Los sockets pueden ser usados para comunicarse con un servidor web,
con un servidor de correo, o con muchos otros tipos de servidores. Todo lo
que se necesita es encontrar el documento que describe el protocolo
correspondiente y escribir el código para enviar y recibir los datos de
acuerdo a ese protocolo.

Sin embargo, como el protocolo que se usa con más frecuencia es el
protocolo web HTTP, Python tiene una librería especial diseñada especialmente
para trabajar con el protocolo HTTP para recibir documentos y datos a través
de la web.

Uno de los requisitos para utilizar el protocolo HTTP es la necesidad de enviar
y recibir datos como objectos binarios, en vez de cadenas. En el ejemplo anterior,
los métodos `encode()` y `decode()` convierten cadenas en objectos binarios
y viceversa.

El siguiente ejemplo utiliza la notación `b''` para especificar que una varialbe
debe ser almacenada como un objeto binario. `encode()` y `b''` son equivalentes.

~~~~
>>> b'Hola mundo'
b'Hola mundo'
>>> 'Hola mundo'.encode()
b'Hola mundo'
~~~~

Recepción de una imagen mediante HTTP
-------------------------------------

\index{imagen!urllib}
\index{jpg!imagen}
\index{jpg}

En el ejemplo anterior, recibimos un archivo de texto sin formato que tenía
saltos de línea en su interior, y lo único que hicimos cuando el programa se
ejecutó fue copiar los datos a la pantalla. Podemos utilizar un programa similar
para recibir una imagen utilizando HTTP. En vez de copiar los datos a la pantalla
conforme va funcionando el programa, vamos a guardar los datos en una cadena,
remover la cabecera, y después guardar los datos de la imagen en un archivo
tal como se muestra a continuación:

\VerbatimInput{../code3/urljpeg.py}

Cuando el programa corre, produce la siguiente salida:

~~~~
$ python urljpeg.py
5120 5120
5120 10240
4240 14480
5120 19600
...
5120 214000
3200 217200
5120 222320
5120 227440
3167 230607
Header length 394
HTTP/1.1 200 OK
Date: Fri, 21 Feb 2020 01:45:41 GMT
Server: Apache/2.4.18 (Ubuntu)
Last-Modified: Mon, 15 May 2017 12:27:40 GMT
ETag: "38342-54f8f2e5b6277"
Accept-Ranges: bytes
Content-Length: 230210
Vary: Accept-Encoding
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: image/jpeg
~~~~

Puedes observar que para esta url, la cabecera `Content-Type`
indica que el cuerpo del documento es una imagen
(`image/jpeg`). Una vez que el programa termina, puedes ver
los datos de la imagen abriendo el archivo `cosa.jpg` en un
visor de imágenes.

Al ejecutar el programa, se puede ver que no se obtienen 5120 caracteres cada vez
que llamamos el método `recv()`. Se obtienen tantos caracteres como hayan sido
transferidos por el sevidor web hacia nosotros a través de la red en el momento
de la llamada a `recv()`. En este emeplo, se obtienen al menos 3200 caracteres cada
vez que solicitamos hasta 5120 caracteres de datos.

Los resultados obtenidos pueden ser diferentes dependiendo de tu velocidad de
internet. Además observa que en la última llamada a `recv()` obtenemos 3167 bytes,
lo cual es el final de la cadena, y en la siguiente llamada a `recv()` obtenemos
una cadena de longitud cero que indica que el servidor ya ha llamado `close()`
en su lado del socket, y por lo tanto no quedan más datos pendientes por recibir.

\index{time}
\index{time.sleep}

Podemos retardar las llamadas sucesivas a `recv()` al descomentar la
llamada a `time.sleep()`. De esta forma, esperamos un cuarto de segundo
después de cada llamada de modo que el servidor puede "adelantarse" a
nosotros y enviarnos más datos antes de que llamemos de nuevo a `recv()`. Con el
retraso, esta vez el programa se ejecuta así:

~~~~
$ python urljpeg.py
5120 5120
5120 10240
5120 15360
...
5120 225280
5120 230400
208 230608
Header length 394
HTTP/1.1 200 OK
Date: Fri, 21 Feb 2020 01:57:31 GMT
Server: Apache/2.4.18 (Ubuntu)
Last-Modified: Mon, 15 May 2017 12:27:40 GMT
ETag: "38342-54f8f2e5b6277"
Accept-Ranges: bytes
Content-Length: 230210
Vary: Accept-Encoding
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: image/jpeg
~~~~

Ahora todas las llamadas a `recv()`, excepto la primera y la última,
nos dan 5120 caracteres cada vez que solicitamos más datos.

Existe un buffer entre el servidor que hace las peticiones `send()`
y nuestra aplicación que hace las peticiones `recv()`. Cuando ejecutamos
el programa con el retraso activado, en algún momento el servidor podría
llenar el buffer del socket y verse forzado a detenerse hasta que nuestro
programa empiece a vaciar ese buffer. La detención de la aplicación que
envía los datos o de la que los recibe se llama "control de flujo".

\index{control de flujo}

Recepción de páginas web con `urllib`
-------------------------------------

Mientras que podemos enviar y recibir datos manualmente mediante HTTP utilizando
la librería socket, existe una forma mucho más simple para realizar esta habitual
tarea en Python, utilizando la librería `urllib`.

Utilizando `urllib`, es posible tratar una página web de forma parecida
a un archivo. Se puede indicar simplemente qué página web se desea recuperar y
`urllib` se encargará de manejar todo lo referente al protocolo HTTP y los detalles
de la cabecera.

El código equivalente para leer el archivo *romeo.txt* desde la web usando
`urllib` es el siguiente:

\VerbatimInput{../code3/urllib1.py}

Una vez que la página web ha sido abierta con `urllib.urlopen`, se puede
tratar como un archivo y leer a través de ella utilizando un bucle `for`.

Cuando el programa se ejecuta, en su salida sólo vemos el contenido del archivo.
Las cabeceras siguen enviándose, pero el código de `urllib` se encarga
de manejar las cabeceras y solamente nos devuelve los datos.

~~~~
But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
~~~~

Como ejemplo, podemos escribir un programa para obtener los datos de 
`romeo.txt` y calcular la frecuencia de cada palabra en el archivo
de la forma siguiente:

\VerbatimInput{../code3/urlwords.py}

De nuevo vemos que, una vez abierta la página web, se puede leer como
si fuera un archivo local.

Leyendo archivos binarios con `urllib`
-------------------------------------

A veces se desea obtener un archivo que no es de texto (o binario) tal
como una imagen o un archivo de video. Los datos en esos archivos generalmente
no son útiles para ser impresos en pantalla, pero se puede hacer fácilmente una
copia de una URL a un archivo local en el disco duro utilizando `urllib`.

\index{archivo binario}

La secuencia es abrir la dirección URL y utilizar `read` para descargar
todo el contenido del documento en una cadena (`img`) para después
escribir esa información a un archivo local, tal como se muestra a continuación:

\VerbatimInput{../code3/curl1.py}

Este programa lee todos los datos que recibe de la red y los almacena
en la variable `img` en la memoria principal de la computadora, después, 
abre el archivo `portada.jpg` y escribe los datos en el disco.
El argumento `wb` en la función `open()` abre un archivo binario en modo
de escritura solamente. Este programa funcionará siempre y cuando el tamaño
del archivo sea menor que el tamaño de la memoria de la computadora.

Aún asi, si es un archivo grande de audio o video, este programa podría
fallar o al menos ejecutar muy muy lento cuando la memoria de la computadora
se haya agotado. Para evitar que la memoria se termine, almacenamos los
datos en bloques (o buffers) y luego escribimos cada bloque en el disco
antes de obtener el siguiente bloque. De esta forma, el programa puede
leer archivos de cualquier tamaño sin utilizar toda la memoria disponible
en la computadora.

\VerbatimInput{../code3/curl2.py}

En este ejemplo, leemos solamente 100,000 caracteres a la vez, y después
los escribimos al archivo `portada.jpg` antes de obtener los
siguientes 100,000 caracteres de datos desde la web.

Este programa se ejecuta como se observa a continuación:

~~~~
python curl2.py
230210 caracteres copiados.
~~~~

Parsing HTML and scraping the web
---------------------------------

\index{web!scraping}
\index{parsing HTML}

One of the common uses of the `urllib` capability in Python
is to *scrape* the web. Web scraping is when we write a
program that pretends to be a web browser and retrieves pages, then
examines the data in those pages looking for patterns.

As an example, a search engine such as Google will look at the source of
one web page and extract the links to other pages and retrieve those
pages, extracting links, and so on. Using this technique, Google
*spiders* its way through nearly all of the pages on the
web.

Google also uses the frequency of links from pages it finds to a
particular page as one measure of how "important" a page is and how high
the page should appear in its search results.

Parsing HTML using regular expressions
--------------------------------------

One simple way to parse HTML is to use regular expressions to repeatedly
search for and extract substrings that match a particular pattern.

Here is a simple web page:

~~~~ {.html}
<h1>The First Page</h1>
<p>
If you like, you can switch to the
<a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>.
</p>
~~~~

We can construct a well-formed regular expression to match and extract
the link values from the above text as follows:

~~~~
href="http[s]?://.+?"
~~~~

Our regular expression looks for strings that start with
"href=\"http://" or "href=\"https://", followed by one or more characters (`.+?`),
followed by another double quote. The question mark behind the `[s]?` indicates
to search for the string "http" followed by zero or one "s". 

The question mark added to the `.+?` indicates
that the match is to be done in a "non-greedy" fashion instead of a
"greedy" fashion. A non-greedy match tries to find the
*smallest* possible matching string and a greedy match
tries to find the *largest* possible matching string.

\index{greedy}
\index{non-greedy}

We add parentheses to our regular expression to indicate which part of
our matched string we would like to extract, and produce the following
program:

\index{regex!parentheses}
\index{parentheses!regular expression}

\VerbatimInput{../code3/urlregex.py}

The `ssl` library allows this program to access web sites that strictly
enforce HTTPS. The `read` method returns HTML source code as a bytes object 
instead of returning an HTTPResponse object. The `findall` regular expression
method will give us a list of all of the strings that match our
regular expression, returning only the link text between the double quotes.

When we run the program and input a URL, we get the following output:

~~~~
Enter - https://docs.python.org
https://docs.python.org/3/index.html
https://www.python.org/
https://docs.python.org/3.8/
https://docs.python.org/3.7/
https://docs.python.org/3.5/
https://docs.python.org/2.7/
https://www.python.org/doc/versions/
https://www.python.org/dev/peps/
https://wiki.python.org/moin/BeginnersGuide
https://wiki.python.org/moin/PythonBooks
https://www.python.org/doc/av/
https://www.python.org/
https://www.python.org/psf/donations/
http://sphinx.pocoo.org/
~~~~

Regular expressions work very nicely when your HTML is well formatted
and predictable. But since there are a lot of "broken" HTML pages out
there, a solution only using regular expressions might either miss some
valid links or end up with bad data.

This can be solved by using a robust HTML parsing library.

Parsing HTML using BeautifulSoup
--------------------------------

\index{BeautifulSoup}

Even though HTML looks like XML^[The XML format is described in the next chapter.]
and some pages are carefully
constructed to be XML, most HTML is generally broken in ways that cause
an XML parser to reject the entire page of HTML as improperly formed.

There are a number of Python libraries which can help you parse HTML and
extract data from the pages. Each of the libraries has its strengths and
weaknesses and you can pick one based on your needs.

As an example, we will simply parse some HTML input and extract links
using the *BeautifulSoup* library. BeautifulSoup tolerates highly flawed
HTML and still lets you easily extract the data you need. You can download and
install the BeautifulSoup code from:

<https://pypi.python.org/pypi/beautifulsoup4>

Information on installing BeautifulSoup with the Python Package Index tool `pip`
is available at:

<https://packaging.python.org/tutorials/installing-packages/>

We will use `urllib` to read the page and then use
`BeautifulSoup` to extract the `href` attributes
from the anchor (`a`) tags.

\index{BeautifulSoup}
\index{HTML}
\index{parsing!HTML}

\VerbatimInput{../code3/urllinks.py}

The program prompts for a web address, then opens the web page, reads
the data and passes the data to the BeautifulSoup parser, and then
retrieves all of the anchor tags and prints out the `href`
attribute for each tag.

When the program runs, it produces the following output:

~~~~
Enter - https://docs.python.org
genindex.html
py-modindex.html
https://www.python.org/
#
whatsnew/3.6.html
whatsnew/index.html
tutorial/index.html
library/index.html
reference/index.html
using/index.html
howto/index.html
installing/index.html
distributing/index.html
extending/index.html
c-api/index.html
faq/index.html
py-modindex.html
genindex.html
glossary.html
search.html
contents.html
bugs.html
about.html
license.html
copyright.html
download.html
https://docs.python.org/3.8/
https://docs.python.org/3.7/
https://docs.python.org/3.5/
https://docs.python.org/2.7/
https://www.python.org/doc/versions/
https://www.python.org/dev/peps/
https://wiki.python.org/moin/BeginnersGuide
https://wiki.python.org/moin/PythonBooks
https://www.python.org/doc/av/
genindex.html
py-modindex.html
https://www.python.org/
#
copyright.html
https://www.python.org/psf/donations/
bugs.html
http://sphinx.pocoo.org/
~~~~

This list is much longer because some HTML anchor tags are relative
paths (e.g., tutorial/index.html) or in-page references (e.g., '#')
that do not include "http://" or "https://", which was a
requirement in our regular expression.

You can use also BeautifulSoup to pull out various parts of each tag:

\VerbatimInput{../code3/urllink2.py}

~~~~
python urllink2.py
Enter - http://www.dr-chuck.com/page1.htm
TAG: <a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>
URL: http://www.dr-chuck.com/page2.htm
Content: ['\nSecond Page']
Attrs: [('href', 'http://www.dr-chuck.com/page2.htm')]
~~~~

`html.parser` is the HTML parser included in the standard Python 3 library.
Information on other HTML parsers is available at:

<http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser>

These examples only begin to show the power of BeautifulSoup when it
comes to parsing HTML.

Bonus section for Unix / Linux users
------------------------------------

If you have a Linux, Unix, or Macintosh computer, you probably have
commands built in to your operating system that retrieves both plain
text and binary files using the HTTP or File Transfer (FTP) protocols.
One of these commands is `curl`:

\index{curl}

~~~~ {.bash}
$ curl -O http://www.py4e.com/cover.jpg
~~~~

The command `curl` is short for "copy URL" and so the two
examples listed earlier to retrieve binary files with `urllib`
are cleverly named `curl1.py` and `curl2.py` on
[www.py4e.com/code3](http://www.py4e.com/code3) as
they implement similar functionality to the `curl` command.
There is also a `curl3.py` sample program that does this task
a little more effectively, in case you actually want to use this pattern
in a program you are writing.

A second command that functions very similarly is `wget`:

\index{wget}

~~~~ {.bash}
$ wget http://www.py4e.com/cover.jpg
~~~~

Both of these commands make retrieving webpages and remote files a
simple task.

Glossary
--------

BeautifulSoup
:   A Python library for parsing HTML documents and extracting data from
    HTML documents that compensates for most of the imperfections in the
    HTML that browsers generally ignore. You can download the
    BeautifulSoup code from [www.crummy.com](http://www.crummy.com).
\index{BeautifulSoup}

port
:   A number that generally indicates which application you are
    contacting when you make a socket connection to a server. As an
    example, web traffic usually uses port 80 while email traffic uses
    port 25.
\index{port}

scrape
:   When a program pretends to be a web browser and retrieves a web
    page, then looks at the web page content. Often programs are
    following the links in one page to find the next page so they can
    traverse a network of pages or a social network.
\index{socket}

socket
:   A network connection between two applications where the applications
    can send and receive data in either direction.
\index{socket}

spider
:   The act of a web search engine retrieving a page and then all the
    pages linked from a page and so on until they have nearly all of the
    pages on the Internet which they use to build their search index.
\index{spider}

Exercises
---------

**Exercise 1: Change the socket program `socket1.py` to prompt
the user for the URL so it can read any web page. You can use
`split('/')` to break the URL into its component parts so you
can extract the host name for the socket `connect` call. Add
error checking using `try` and `except` to handle
the condition where the user enters an improperly formatted or
non-existent URL.**

**Exercise 2: Change your socket program so that it counts the number of
characters it has received and stops displaying any text after it has
shown 3000 characters. The program should retrieve the entire document
and count the total number of characters and display the count of the
number of characters at the end of the document.**

**Exercise 3: Use `urllib` to replicate the previous exercise
of (1) retrieving the document from a URL, (2) displaying up to 3000
characters, and (3) counting the overall number of characters in the
document. Don't worry about the headers for this exercise, simply show
the first 3000 characters of the document contents.**

**Exercise 4: Change the `urllinks.py` program to extract and
count paragraph (p) tags from the retrieved HTML document and display
the count of the paragraphs as the output of your program. Do not
display the paragraph text, only count them. Test your program on
several small web pages as well as some larger web pages.**

**Exercise 5: (Advanced) Change the socket program so that it only shows
data after the headers and a blank line have been received. Remember
that `recv` receives characters (newlines and all), not lines.**

