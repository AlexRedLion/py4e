Programas en red
================

Aunque muchos de los ejemplos en este libro se han enfocado en leer archivos
y buscar datos en ellos, hay muchas fuentes de información diferentes
si se tiene en cuenta el Internet.

En este capítulo fingiremos ser un navegador web y recuperaremos páginas
web utilizando el Protocolo de Transporte de Hipertexto (HyperText Transfer Protocol - HTTP). Luego revisaremos los datos de esas páginas web y
los analizaremos.

Protocolo de Transporte de Hipertexto - HTTP
--------------------------------------------

El protocolo de red que hace funcionar la web es en realidad bastante
simple, y existe un soporte integrado en Python llamado `sockets`, el
cual hace que resulte muy fácil realizar conexiones de red y recuperar
datos a través de esas conexiones desde un programa de Python.

Un socket es muy parecido a un archivo, a excepción de que 
proporciona una conexión de doble sentido entre dos programas. Es posible
tanto leer como escribir en un mismo socket. Si se escribe algo en un
socket, es enviado a la aplicación que está al otro lado de éste. Si
se lee desde el socket, se obtienen los datos que la otra aplicación
ha enviado.

Pero si intentas leer un socket cuando el programa que está del otro lado
del socket no ha enviado ningún dato, puedes sentarte y esperar. Si los programas
en ambos extremos del socket simplemente esperan por datos sin enviar nada,
van a esperar por mucho, mucho tiempo, así que una parte importante de 
los programas que se comunican a través de internet consiste en tener algún tipo
de protocolo.

Un protocolo es un conjunto de reglas precisas que determinan quién va primero,
qué debe hacer, cuáles son las respuestas siguientes para ese mensaje, quién
envía a continuación, etcétera. En cierto sentido las aplicaciones
a ambos lados del socket están interpretando un baile y cada una
debe estar segura de no pisar los pies de la otra.

Hay muchos documentos que describen estos protocolos de red. El
Protocolo de Transporte de Hipertext está descrito en el siguiente documento:

<https://www.w3.org/Protocols/rfc2616/rfc2616.txt>

Se trata de un documento largo y complejo de 176 páginas, con un montón de
detalles. Si lo encuentras interesante, siéntete libre de leerlo completo. Pero si
echas un vistazo alrededor de la página 36 del RFC2616, encontrarás la sintaxis para las
peticiones GET. Para pedir un documento a un servidor web, hacemos una conexión al
servidor `www.pr4e.org` en el puerto 80, y luego enviamos una línea como esta:

`GET http://data.pr4e.org/romeo.txt HTTP/1.0 `

en la cual el segundo parámetro es la página web que estamos solicitando, y a
continuación enviamos una línea en blanco. El servidor web responderá con una
cabecera que contiene información acerca del documento y una línea en blanco,
seguido por el contenido del documento.

El navegador web más sencillo del mundo
---------------------------------------

Quizá la manera más sencilla de demostrar cómo funciona el protocolo HTTP sea
escribir un programa en Python muy sencillo, que realice una conexión a un
servidor web y siga las reglas del protocolo HTTP para solicitar un documento
y mostrar lo que el servidor envía de regreso.

\VerbatimInput{../code3/socket1.py}

En primer lugar, el programa realiza una conexión al puerto 80 del servidor
[www.py4e.com](http://www.py4e.com). Puesto que nuestro programa está jugando
el rol de "navegador web", el protocolo HTTP dice que debemos enviar
el comando GET seguido de una línea en blanco. `\r\n` representa un
salto de línea (end of line, o EOL en inglés), así que `\r\n\r\n`
significa que no hay nada entre dos secuencias de salto de línea. Ese
es el equivalente de una línea en blanco.

![Conexión de un socket](height=2.0in@../images/socket)

Una vez que enviamos esa línea en blanco, escribimos un bucle que recibe
los datos desde el socket en bloques de 512 caracteres y los imprime en
pantalla hasta que no quedan más datos por leer (es decir, la llamada a
recv() devuelve una cadena vacía).

El programa produce la salida siguiente:

~~~~
HTTP/1.1 200 OK
Date: Wed, 11 Apr 2018 18:52:55 GMT
Server: Apache/2.4.7 (Ubuntu)
Last-Modified: Sat, 13 May 2017 11:22:22 GMT
ETag: "a7-54f6609245537"
Accept-Ranges: bytes
Content-Length: 167
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: text/plain

But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
~~~~

La salida comienza con la cabecera que el servidor envía para describir
el documento. Por ejemplo, la cabecera `Content-Type` indica que
el documento es un documento de texto sin formato (`text/plain`).

Después de que el servidor nos envía la cabecera, añade una línea en blanco
para indicar el final de la cabecera, y después envía los datos reales del
archivo *romeo.txt*.

Este ejemplo muestra cómo hacer una conexión de red de bajo nivel con
sockets. Los sockets pueden ser usados para comunicarse con un servidor web,
con un servidor de correo, o con muchos otros tipos de servidores. Todo lo
que se necesita es encontrar el documento que describe el protocolo
correspondiente y escribir el código para enviar y recibir los datos de
acuerdo a ese protocolo.

Sin embargo, como el protocolo que se usa con más frecuencia es el
protocolo web HTTP, Python tiene una librería especial diseñada especialmente
para trabajar con éste para recibir documentos y datos a través
de la web.

Uno de los requisitos para utilizar el protocolo HTTP es la necesidad de enviar
y recibir datos como objectos binarios, en vez de cadenas. En el ejemplo anterior,
los métodos `encode()` y `decode()` convierten cadenas en objectos binarios
y viceversa.

El siguiente ejemplo utiliza la notación `b''` para especificar que una variable
debe ser almacenada como un objeto binario. `encode()` y `b''` son equivalentes.

~~~~
>>> b'Hola mundo'
b'Hola mundo'
>>> 'Hola mundo'.encode()
b'Hola mundo'
~~~~

Recepción de una imagen mediante HTTP
-------------------------------------

\index{imagen!urllib}
\index{jpg!imagen}
\index{jpg}

En el ejemplo anterior, recibimos un archivo de texto sin formato que tenía
saltos de línea en su interior, y lo único que hicimos cuando el programa se
ejecutó fue copiar los datos a la pantalla. Podemos utilizar un programa similar
para recibir una imagen utilizando HTTP. En vez de copiar los datos a la pantalla
conforme va funcionando el programa, vamos a guardar los datos en una cadena,
remover la cabecera, y después guardar los datos de la imagen en un archivo
tal como se muestra a continuación:

\VerbatimInput{../code3/urljpeg.py}

Cuando el programa corre, produce la siguiente salida:

~~~~
$ python urljpeg.py
5120 5120
5120 10240
4240 14480
5120 19600
...
5120 214000
3200 217200
5120 222320
5120 227440
3167 230607
Header length 394
HTTP/1.1 200 OK
Date: Fri, 21 Feb 2020 01:45:41 GMT
Server: Apache/2.4.18 (Ubuntu)
Last-Modified: Mon, 15 May 2017 12:27:40 GMT
ETag: "38342-54f8f2e5b6277"
Accept-Ranges: bytes
Content-Length: 230210
Vary: Accept-Encoding
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: image/jpeg
~~~~

Puedes observar que para esta url, la cabecera `Content-Type`
indica que el cuerpo del documento es una imagen
(`image/jpeg`). Una vez que el programa termina, puedes ver
los datos de la imagen abriendo el archivo `cosa.jpg` en un
visor de imágenes.

Al ejecutar el programa, se puede ver que no se obtienen 5120 caracteres cada vez
que llamamos el método `recv()`. Se obtienen tantos caracteres como hayan sido
transferidos por el servidor web hacia nosotros a través de la red en el momento
de la llamada a `recv()`. En este emeplo, se obtienen al menos 3200 caracteres cada
vez que solicitamos hasta 5120 caracteres de datos.

Los resultados pueden variar dependiendo de tu velocidad de
internet. Además, observa que en la última llamada a `recv()` obtenemos 3167 bytes,
lo cual es el final de la cadena, y en la siguiente llamada a `recv()` obtenemos
una cadena de longitud cero que indica que el servidor ya ha llamado `close()`
en su lado del socket, y por lo tanto no quedan más datos pendientes por recibir.

\index{time}
\index{time.sleep}

Podemos retardar las llamadas sucesivas a `recv()` al descomentar la
llamada a `time.sleep()`. De esta forma, esperamos un cuarto de segundo
después de cada llamada de modo que el servidor puede "adelantarse" a
nosotros y enviarnos más datos antes de que llamemos de nuevo a `recv()`. Con el
retraso, esta vez el programa se ejecuta así:

~~~~
$ python urljpeg.py
5120 5120
5120 10240
5120 15360
...
5120 225280
5120 230400
208 230608
Header length 394
HTTP/1.1 200 OK
Date: Fri, 21 Feb 2020 01:57:31 GMT
Server: Apache/2.4.18 (Ubuntu)
Last-Modified: Mon, 15 May 2017 12:27:40 GMT
ETag: "38342-54f8f2e5b6277"
Accept-Ranges: bytes
Content-Length: 230210
Vary: Accept-Encoding
Cache-Control: max-age=0, no-cache, no-store, must-revalidate
Pragma: no-cache
Expires: Wed, 11 Jan 1984 05:00:00 GMT
Connection: close
Content-Type: image/jpeg
~~~~

Ahora todas las llamadas a `recv()`, excepto la primera y la última,
nos dan 5120 caracteres cada vez que solicitamos más datos.

Existe un buffer entre el servidor que hace las peticiones `send()`
y nuestra aplicación que hace las peticiones `recv()`. Cuando ejecutamos
el programa con el retraso activado, en algún momento el servidor podría
llenar el buffer del socket y verse forzado a detenerse hasta que nuestro
programa empiece a vaciar ese buffer. La detención de la aplicación que
envía los datos o de la que los recibe se llama "control de flujo".

\index{control de flujo}

Recepción de páginas web con `urllib`
-------------------------------------

Aunque podemos enviar y recibir datos manualmente mediante HTTP utilizando
la librería socket, existe una forma mucho más simple para realizar esta habitual
tarea en Python, utilizando la librería `urllib`.

Utilizando `urllib`, es posible tratar una página web de forma parecida
a un archivo. Se puede indicar simplemente qué página web se desea recuperar y
`urllib` se encargará de manejar todos los detalles referentes al protocolo HTTP y 
a la cabecera.

El código equivalente para leer el archivo *romeo.txt* desde la web usando
`urllib` es el siguiente:

\VerbatimInput{../code3/urllib1.py}

Una vez que la página web ha sido abierta con `urllib.urlopen`, se puede
tratar como un archivo y leer a través de ella utilizando un bucle `for`.

Cuando el programa se ejecuta, en su salida sólo vemos el contenido del archivo.
Las cabeceras siguen enviándose, pero el código de `urllib` se encarga
de manejarlas y solamente nos devuelve los datos.

~~~~
But soft what light through yonder window breaks
It is the east and Juliet is the sun
Arise fair sun and kill the envious moon
Who is already sick and pale with grief
~~~~

Como ejemplo, podemos escribir un programa para obtener los datos de 
`romeo.txt` y calcular la frecuencia de cada palabra en el archivo
de la forma siguiente:

\VerbatimInput{../code3/urlwords.py}

De nuevo vemos que, una vez abierta la página web, se puede leer como
si fuera un archivo local.

Leyendo archivos binarios con `urllib`
-------------------------------------

A veces se desea obtener un archivo que no es de texto (o binario) tal
como una imagen o un archivo de video. Los datos en esos archivos generalmente
no son útiles para ser impresos en pantalla, pero se puede hacer fácilmente una
copia de una URL a un archivo local en el disco duro utilizando `urllib`.

\index{archivo binario}

El método consiste en abrir la dirección URL y utilizar `read` para descargar
todo el contenido del documento en una cadena (`img`) para después
escribir esa información a un archivo local, tal como se muestra a continuación:

\VerbatimInput{../code3/curl1.py}

Este programa lee todos los datos que recibe de la red y los almacena
en la variable `img` en la memoria principal de la computadora. Después, 
abre el archivo `portada.jpg` y escribe los datos en el disco.
El argumento `wb` en la función `open()` abre un archivo binario en modo
de escritura solamente. Este programa funcionará siempre y cuando el tamaño
del archivo sea menor que el tamaño de la memoria de la computadora.

Aún asi, si es un archivo grande de audio o video, este programa podría
fallar o al menos ejecutarse sumamente lento cuando la memoria de la computadora
se haya agotado. Para evitar que la memoria se termine, almacenamos los
datos en bloques (o buffers) y luego escribimos cada bloque en el disco
antes de obtener el siguiente bloque. De esta forma, el programa puede
leer archivos de cualquier tamaño sin utilizar toda la memoria disponible
en la computadora.

\VerbatimInput{../code3/curl2.py}

En este ejemplo, leemos solamente 100,000 caracteres a la vez, y después
los escribimos al archivo `portada.jpg` antes de obtener los
siguientes 100,000 caracteres de datos desde la web.

Este programa se ejecuta como se observa a continuación:

~~~~
python curl2.py
230210 caracteres copiados.
~~~~

Análisis the HTML y rascado de la web
-------------------------------------

\index{rascado!web}
\index{análisis HTML}

Uno de los usos más comunes de las capacidades de `urllib` en Python
es *rascar* la web. El rascado de la web es cuando escribimos un programa
que pretende ser un navegador web y recupera páginas, examinando
luego los datos de esas páginas para encontrar ciertos patrones.

Por ejemplo, un motor de búsqueda como Google buscará el código de una
página web, extraerá los enlaces a otras paginas y las recuperará,
extrayendo los enlaces que haya en ellas y así sucesivamente. Utilizando
esta técnica, las *arañas* de Google alcanzan a casi todas las páginas
de la web.

Google utiliza también la frecuencia con que las páginas que encuentra enlazan
hacia una página concreta para calcular la "importancia" de esa página, y la
posición en la que debe aparecer dentro de sus resultados de búsqueda.

Análisis de HTML mediante expresiones regulares
-----------------------------------------------

Una forma sencilla de analizar HTML consiste en utilizar expresiones regulares para
hacer búsquedas repetitivas que extraigan subcadenas coincidentes con un patrón en
particular.

Aquí tenemos una página web simple:

~~~~ {.html}
<h1>La Primera Página</h1>
<p>
Si quieres, puedes visitar la
<a href="http://www.dr-chuck.com/page2.htm">
Segunda Página</a>.
</p>
~~~~

Podemos construir una expresión regular bien formada para buscar y extraer
los valores de los enlaces del texto anterior, de esta forma:

~~~~
href="http[s]?://.+?"
~~~~

Nuestra expresión regular busca cadenas que comiencen con
"href=\"http://" o "href=\"https://", seguido de uno o más caracteres (`.+?`),
seguidos por otra comilla doble. El signo de interrogación después de `[s]?`
indica que la coincidencia debe ser hecha en modo "no-codicioso", en vez de en
modo "codicioso". Una búsqueda no-codiciosa intenta encontrar la cadena
coincidente *más pequeña* posible, mientras que una búsqueda codiciosa intentaría
localizar la cadena coincidente *más grande*.

\index{codicioso}
\index{no-codicioso}

Añadimos paréntesis a nuestra expresión regular para indicar qué parte de la
cadena localizada queremos extraer, y obtenemos el siguiente programa:

\index{regex!paréntesis}
\index{paréntesis!expresión regular}

\VerbatimInput{../code3/urlregex.py}

La librería `ssl` permite a nuestro programa acceder a los sitios web que
estrictamente requieren HTTPS. El método `read` devuelve código fuente en HTML
como un objeto binario en vez de devolver un objeto HTTPResponse. El método
de expresiones regulares `findall` nos da una lista de todas las cadenas que
coinciden con la expresión regular, devolviendo solamente el texto del enlace
entre las comillas dobles.

Cuando corremos el programa e ingresamos una URL, obtenemos lo siguiente:

~~~~
Introduzca - https://docs.python.org
https://docs.python.org/3/index.html
https://www.python.org/
https://devguide.python.org/docquality/#helping-with-documentation
https://docs.python.org/3.9/
https://docs.python.org/3.8/
https://docs.python.org/3.7/
https://docs.python.org/3.6/
https://docs.python.org/3.5/
https://docs.python.org/2.7/
https://www.python.org/doc/versions/
https://www.python.org/dev/peps/
https://wiki.python.org/moin/BeginnersGuide
https://wiki.python.org/moin/PythonBooks
https://www.python.org/doc/av/
https://devguide.python.org/
https://www.python.org/
https://www.python.org/psf/donations/
https://docs.python.org/3/bugs.html
https://www.sphinx-doc.org/
~~~~

Las expresiones regulares funcionan muy bien cuando el HTML está bien formateado
y es predecible. Pero dado que ahí afuera hay muchas páginas con HTML "defectuoso",
una solución que solo utilice expresiones regulares podría perder algunos
enlaces válidos, o bien terminar obteniendo datos erróneos.

Esto se puede resolver utilizando una librería robusta de análisis de HTML.

Análisis de HTML mediante BeautifulSoup
---------------------------------------

\index{BeautifulSoup}

A pesar de que HTML es parecido a XML^[El formato XML es descrito en el siguiente
capítulo.] y que algunas páginas son construidas cuidadosamente para ser XML, la
mayoría del HTML generalmente está incompleto, de modo que puede causar que un analizador
de XML rechace una página HTML completa por estar formada inadecuadamente.

Hay varias librerías en Python que pueden ayudarte a
analizar HTML y extraer datos de las páginas. Cada una tiene
sus fortalezas y debilidades, por lo que puedes elegir una basada en tus necesidades.

Por ejemplo, vamos a analizar una entrada HTML cualquiera y a extraer
enlaces utilizando la librería *BeautifulSoup*. BeautifulSoup tolera código HTML bastante
defectuoso y aún así te deja extraer los datos que necesitas. Puedes descargar e instalar
el código de BeautifulSoup desde:

<https://pypi.python.org/pypi/beautifulsoup4>

La información acerca de la instalación de BeautifulSoup utilizando la herramienta de
Python Package Index (Índice de Paquete de Python) `pip`, disponible en:

<https://packaging.python.org/tutorials/installing-packages/>

Vamos a utilizar `urllib` para leer la página y después utilizaremos
`BeautifulSoup` para extraer los atributos `href`
de las etiquetas de anclaje (`a`).

\index{BeautifulSoup}
\index{HTML}
\index{análisis!HTML}

\VerbatimInput{../code3/urllinks.py}

El programa solicita una dirección web, luego la abre, lee
los datos y se los pasa al analizador BeautifulSoup. Luego, recupera todas las
etiquetas de anclaje e imprime en pantalla el atributo `href`
de cada una de ellas.

Cuando el programa se ejecuta, produce lo siguiente:

~~~~
Introduzca - https://docs.python.org
genindex.html
py-modindex.html
https://www.python.org/
#
whatsnew/3.8.html
whatsnew/index.html
tutorial/index.html
library/index.html
reference/index.html
using/index.html
howto/index.html
installing/index.html
distributing/index.html
extending/index.html
c-api/index.html
faq/index.html
py-modindex.html
genindex.html
glossary.html
search.html
contents.html
bugs.html
https://devguide.python.org/docquality/#helping-with-documentation
about.html
license.html
copyright.html
download.html
https://docs.python.org/3.9/
https://docs.python.org/3.8/
https://docs.python.org/3.7/
https://docs.python.org/3.6/
https://docs.python.org/3.5/
https://docs.python.org/2.7/
https://www.python.org/doc/versions/
https://www.python.org/dev/peps/
https://wiki.python.org/moin/BeginnersGuide
https://wiki.python.org/moin/PythonBooks
https://www.python.org/doc/av/
https://devguide.python.org/
genindex.html
py-modindex.html
https://www.python.org/
#
copyright.html
https://www.python.org/psf/donations/
https://docs.python.org/3/bugs.html
https://www.sphinx-doc.org/
~~~~

Esta lista es mucho más larga porque algunas de las etiquetas de anclaje son
rutas relativas (e.g., tutorial/index.html) o referencias dentro de la página (p. ej., '#')
que no incluyen "http://" o "https://", lo cual era un requerimiento en
nuestra expresión regular.

También puedes utilizar BeautifulSoup para extraer varias partes de cada etiqueta de este modo:

\VerbatimInput{../code3/urllink2.py}

~~~~
python urllink2.py
Introduzca - http://www.dr-chuck.com/page1.htm
ETIQUETA: <a href="http://www.dr-chuck.com/page2.htm">
Second Page</a>
URL: http://www.dr-chuck.com/page2.htm
Contenido:
Second Page
Atributos: {'href': 'http://www.dr-chuck.com/page2.htm'}
~~~~

`html.parser` es el analizador de HTML incluido en la librería estándar de
Python 3. Para más información acerca de otros analizadores de HTML, lee:

<http://www.crummy.com/software/BeautifulSoup/bs4/doc/#installing-a-parser>

Estos ejemplo tan sólo muestran un poco de la potencia de BeautifulSoup
cuando se trata de analizar HTML.

Sección extra para usuarios de Unix / Linux
-------------------------------------------

Si tienes una computadora Linux, Unix, o Macintosh, probablemente tienes
comandos nativos de tu sistema operativo para obtener tanto archivos de texto
plano como archivos binarios utilizando los procolos HTTP o de Transferencia
de Archivos (File Transfer - FTP).
Uno de esos comandos es `curl`:

\index{curl}

~~~~ {.bash}
$ curl -O http://www.py4e.com/cover.jpg
~~~~

El comando `curl` es una abreviación de "copiar URL" y por esa razón los
dos ejemplos vistos anteriormente para obtener archivos binarios con `urllib`
son astutamente llamados `curl1.py` y `curl2.py` en
[www.py4e.com/code3](http://www.py4e.com/code3) debido a que ellos
implementan una funcionalidad similar a la del comando `curl`.
Existe también un programa de ejemplo `curl3.py` que realiza la misma tarea de
forma un poco más eficiente, en caso de que quieras usar de verdad este diseño
en algún programa que estés escribiendo.

Un segundo comando que funciona de forma similar es `wget`:

\index{wget}

~~~~ {.bash}
$ wget http://www.py4e.com/cover.jpg
~~~~

Ambos comandos hacen que obtener páginas web y archivos remotos
se vuelva una tarea fácil.

Glosario
--------

BeautifulSoup
:   Una librería de Python para analizar documentos HTML y extraer datos
    de ellos, que compensa la mayoría de las imperfecciones que los
    navegadores HTML normalmente ignoran. Puedes descargar el código de
    BeautifulSoup desde [www.crummy.com](http://www.crummy.com).
\index{BeautifulSoup}

puerto
:   Un número que generalmente indica qué aplicación estás contactando
    cuando realizas una conexión con un socket en un servidor. Por ejemplo,
    el tráfico web normalmente usa el puerto 80, mientras que el tráfico del
    correo electrónico usa el puerto 25.
\index{puerto}

rascado
:   Cuando un programa simula ser un navegador web y recupera una página web,
    para luego realizar una búsqueda en su contenido. A menudo los programas
    siguen los enlaces en una página para encontrar la siguiente, de modo
    que pueden atravesar una red de páginas o una red social.
\index{rascado}

rastrear
:   La acción de un motor de búsqueda web que consiste en recuperar una
    página y luego todas las páginas enlazadas por ella, continuando así
    sucesivamente hasta que tienen casi todas las páginas de Internet, que
    usan a continuación para construir su índice de búsqueda.
\index{rastrear}

socket
:   Una conexión de red entre dos aplicaciones, en la cual dichas
    aplicaciones pueden enviar y recibir datos en ambas direcciones.
\index{socket}



Ejercicios
----------

**Ejercicio 1: Cambia el programa del socket `socket1.py` para que le pida
al usuario la URL, de modo que pueda leer cualquier página web. Puedes usar
`split('/')` para dividir la URL en las partes que la componen, de modo que
puedas extraer el nombre del host para la llamada a `connect` del socket.
Añade comprobación de errores utilizando `try` y `except` para contemplar la
posibilidad de que el usuario introduzca una URL mal formada o
inexistente.**

**Ejercicio 2: Cambia el programa del socket para que cuente el número
de caracteres que ha recibido y se detenga, con un texto en pantalla,
después de que se hayan mostrado 3000 caracteres. El programa debe recuperar
el documento completo y contar el número total de caracteres, mostrando
ese total al final del documento.**

**Ejercicio 3: Utiliza `urllib` para rehacer el ejercicio anterior de modo
que (1) reciba el documento de una URL, (2) muestre hasta 3000 caracteres,
y (3) cuente la cantidad total de caracteres en el documento. No te preocupes
de las cabeceras en este ejercicio, simplemente muesta los primeros 3000
caracteres del contenido del documento.**

**Ejercicio 4: Cambia el programa `urllinks.py` para extraer y contar las
etiquetas de párrafo (p) del documento HTML recuperado y mostrar el total de
párrafos como salida del programa. No muestres el texto de los párrafos, sólo
cuéntalos. Prueba el programa en varias páginas web pequeñas, y también en
otras más grandes.**

**Ejercicio 5: (Avanzado) Cambia el programa del socket de modo que solamente
muestra datos después de que se haya recibido la cabecera y la línea en blanco.
Recuerda que `recv` recibe caracteres (saltos de línea incluidos), no líneas.**

